{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# \n",
    "#    CNOTExRec trainer. Use 4 RNNs with 1 LSTM cell to train X & Z at same time.\n",
    "#\n",
    "#    Copyright (C) 2017 Pooya Ronagh\n",
    "# \n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "from builtins import range\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from util import y2indicator\n",
    "import threading\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from time import localtime, strftime, clock\n",
    "import cPickle as pickle\n",
    "\n",
    "# The CSS code generator matrix\n",
    "G= np.matrix([[0,0,0,1,1,1,1], \\\n",
    "              [0,1,1,0,0,1,1], \\\n",
    "              [1,0,1,0,1,0,1]]).astype(np.int32)\n",
    "error_keys= ['errX3', 'errX4', 'errZ3', 'errZ4']\n",
    "syndrome_keys= ['synX12', 'synX34', 'synZ12', 'synZ34']\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        raw_data, p, lu_avg, lu_std, data_size = get_data(datafolder + filename)\n",
    "        total_size= np.shape(raw_data['synX12'])[0]\n",
    "        test_size= int(test_fraction * total_size)\n",
    "        train_data, test_data = io_data_factory(raw_data, test_size)\n",
    "        self.total_size = total_size\n",
    "        self.p = p\n",
    "        self.lu_avg = lu_avg\n",
    "        self.lu_std = lu_std\n",
    "        self.data_size = data_size\n",
    "        self.test_size = test_size\n",
    "        self.train_data= train_data\n",
    "        self.test_data = test_data\n",
    "        self.train_size= total_size - test_size\n",
    "        self.error_scale= 1.0*total_size/data_size\n",
    "\n",
    "class Data:\n",
    "\n",
    "    def __init__(self, data, train_mode=True):\n",
    "        self.input= {}\n",
    "        self.vir_output= {}\n",
    "        self.vir_output_ind= {}\n",
    "\n",
    "        synX= np.concatenate((data['synX12'], data['synX34']), axis= 1).reshape(-1, 2, 6)\n",
    "        synZ= np.concatenate((data['synZ12'], data['synZ34']), axis= 1).reshape(-1, 2, 6)\n",
    "        self.input['errX3']= synX\n",
    "        self.input['errX4']= synX\n",
    "        self.input['errZ3']= synZ\n",
    "        self.input['errZ4']= synZ\n",
    "\n",
    "        rep_X1= lookup_correction(data['synX12'][:,0:3])\n",
    "        rep_X2= lookup_correction(data['synX12'][:,3:6])\n",
    "        rep_Z1= lookup_correction(data['synZ12'][:,0:3])\n",
    "        rep_Z2= lookup_correction(data['synZ12'][:,3:6])\n",
    "        rep_X3= lookup_correction(data['synX34'][:,0:3])\n",
    "        rep_X4= lookup_correction(data['synX34'][:,3:6])\n",
    "        rep_Z3= lookup_correction(data['synZ34'][:,0:3])\n",
    "        rep_Z4= lookup_correction(data['synZ34'][:,3:6])\n",
    "\n",
    "        self.rec= {}\n",
    "        self.rec['errX3']= (data['errX3'] + rep_X1 + \\\n",
    "            lookup_correction_from_err((rep_X1 + rep_X3) % 2)) % 2\n",
    "        self.rec['errZ3']= (data['errZ3'] + rep_Z1 + rep_Z2 + \\\n",
    "            lookup_correction_from_err((rep_Z1 + rep_Z2 + rep_Z3) % 2)) % 2\n",
    "        self.rec['errX4']= (data['errX4'] + rep_X1 + rep_X2 + \\\n",
    "            lookup_correction_from_err((rep_X1 + rep_X2 + rep_X4) % 2)) % 2    \n",
    "        self.rec['errZ4']= (data['errZ4'] + rep_Z3 + \\\n",
    "            lookup_correction_from_err((rep_Z3 + rep_Z4) % 2)) % 2\n",
    "\n",
    "        for key in error_keys:\n",
    "            self.vir_output[key] = np.array(\\\n",
    "                np.sum((self.rec[key] + lookup_correction_from_err(self.rec[key])) % 2, axis= 1) % 2).transpose()\n",
    "            \n",
    "        for key in error_keys:\n",
    "            self.vir_output_ind[key]=\\\n",
    "            y2indicator(self.vir_output[key],2**1).astype(np.int8)\n",
    "        \n",
    "        if (train_mode):\n",
    "            self.rec= None\n",
    "        else:\n",
    "            self.vir_output = None\n",
    "            self.vir_output_ind= None\n",
    "            \n",
    "def io_data_factory(data, test_size):\n",
    "\n",
    "    train_data_arg = {key:data[key][:-test_size,] for key in data.keys()}\n",
    "    test_data_arg  = {key:data[key][-test_size:,] for key in data.keys()}\n",
    "    train_data = Data(train_data_arg)\n",
    "    test_data = Data(test_data_arg, False)\n",
    "    return train_data, test_data\n",
    "\n",
    "def syndrome(err):\n",
    "    return np.dot(err, G.transpose()) % 2\n",
    "\n",
    "def lookup_correction(syn):\n",
    "    correction_index= np.dot(syn, [[4], [2], [1]]) - 1\n",
    "    return y2indicator(correction_index, 7)\n",
    "\n",
    "def lookup_correction_from_err(err):\n",
    "    syn= syndrome(err)\n",
    "    return lookup_correction(syn)\n",
    "\n",
    "def find_logical_fault(err):\n",
    "\n",
    "    syndrome= np.dot(G, err.transpose()) % 2\n",
    "    correction_index= np.dot([[4, 2, 1]], syndrome.transpose()) - 1\n",
    "    correction = y2indicator(correction_index, 7)\n",
    "    coset= (err + correction) % 2\n",
    "    logical_err= np.sum(coset) % 2\n",
    "    return logical_err\n",
    "\n",
    "def num_logical_fault(prediction, test_data):\n",
    "\n",
    "    error_counter= 0.0\n",
    "    for i in range(len(prediction[error_keys[0]])):\n",
    "        for key in error_keys:\n",
    "            if (find_logical_fault(prediction[key][i] * np.ones(7) + test_data.rec[key][i] % 2)):\n",
    "                error_counter+=1\n",
    "                break\n",
    "    return error_counter/len(prediction[error_keys[0]])\n",
    "\n",
    "def get_data(filename):\n",
    "\n",
    "    data= {}\n",
    "    for key in syndrome_keys:\n",
    "        data[key]= []\n",
    "    for key in error_keys:\n",
    "        data[key]= []\n",
    "    with open(filename) as file:\n",
    "        first_line = file.readline();\n",
    "        p, lu_avg, lu_std, data_size = first_line.split(' ')\n",
    "        p= float(p)\n",
    "        lu_avg= float(lu_avg)\n",
    "        lu_std= float(lu_std)\n",
    "        data_size= int(data_size)\n",
    "        for line in file.readlines():\n",
    "            line_list= line.split(' ')\n",
    "            data['synX12'].append([bit for bit in ''.join(line_list[0:2])])\n",
    "            data['synX34'].append([bit for bit in ''.join(line_list[2:4])])\n",
    "            data['synZ12'].append([bit for bit in ''.join(line_list[8:10])])\n",
    "            data['synZ34'].append([bit for bit in ''.join(line_list[10:12])])\n",
    "            data['errX3'].append([int(line_list[6],2)])\n",
    "            data['errX4'].append([int(line_list[7],2)])\n",
    "            data['errZ3'].append([int(line_list[14],2)])\n",
    "            data['errZ4'].append([int(line_list[15],2)])\n",
    "    for key in data.keys():\n",
    "        data[key]= np.array(data[key]).astype(np.int8)\n",
    "    return data, p, lu_avg, lu_std, data_size\n",
    "\n",
    "def train(param, train_data, test_data, \\\n",
    "          num_classes, num_inputs, input_size, n_batches):\n",
    "\n",
    "    prediction= {}\n",
    "    verbose= param['usr']['verbose']\n",
    "    batch_size= param['opt']['batch size']\n",
    "    learning_rate= param['opt']['learning rate']\n",
    "    num_iterations= param['opt']['iterations']\n",
    "    momentum_val= param['opt']['momentum']\n",
    "    decay_rate= param['opt']['decay']\n",
    "    num_hidden= param['nn']['num hidden'] \n",
    "    W_std= param['nn']['W std'] \n",
    "    b_std= param['nn']['b std'] \n",
    "\n",
    "    # define all parts of the tf graph\n",
    "    tf.reset_default_graph()\n",
    "    x = {}\n",
    "    y = {}\n",
    "    lstm = {}\n",
    "    lstmOut = {}\n",
    "    W= {}\n",
    "    b= {}\n",
    "    logits= {}\n",
    "    loss= {}\n",
    "    predict= {}\n",
    "    \n",
    "    for key in error_keys:\n",
    "        with tf.variable_scope(key):\n",
    "\n",
    "            x[key] = tf.placeholder(tf.float32, [None, num_inputs, input_size])\n",
    "            y[key] = tf.placeholder(tf.float32, [None, num_classes])\n",
    "            lstm[key] = tf.contrib.rnn.LSTMCell(num_hidden)\n",
    "            lstmOut[key], _ = tf.nn.dynamic_rnn(\\\n",
    "                lstm[key], x[key], dtype=tf.float32)\n",
    "            W[key]= tf.Variable(\\\n",
    "                tf.random_normal([num_hidden,num_classes], stddev=W_std))\n",
    "            b[key]= tf.Variable(tf.random_normal([num_classes], stddev=b_std))\n",
    "            logits[key]= tf.matmul(lstmOut[key][:,-1,:], W[key]) + b[key]\n",
    "            loss[key]= tf.nn.softmax_cross_entropy_with_logits(\\\n",
    "                logits=logits[key], labels=y[key])\n",
    "            predict[key]= tf.argmax(logits[key], 1)\n",
    "    \n",
    "    cost= tf.reduce_sum(sum(loss[key] for key in error_keys))\n",
    "    train = tf.train.RMSPropOptimizer(\\\n",
    "        learning_rate, decay=decay_rate, momentum=momentum_val).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    costs= []\n",
    "\n",
    "    with tf.Session() as session:\n",
    "        if (verbose): print('session begins '),\n",
    "        session.run(init)\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            if (verbose): print('.'),\n",
    "\n",
    "            for j in range(n_batches):\n",
    "                beg= j * batch_size\n",
    "                end= j * batch_size + batch_size\n",
    "                feed_dict={}\n",
    "                for key in error_keys:\n",
    "                    feed_dict[x[key]]= train_data.input[key][beg:end,]\n",
    "                    feed_dict[y[key]]= train_data.vir_output_ind[key][beg:end,]\n",
    "                session.run(train, feed_dict)\n",
    "\n",
    "            if (verbose>1):\n",
    "                feed_dict={}\n",
    "                for key in error_keys:\n",
    "                    feed_dict[x[key]]= test_data.input[key]\n",
    "                    feed_dict[y[key]]= test_data.vir_output_ind[key]\n",
    "                test_cost = session.run(cost, feed_dict)\n",
    "                costs.append(test_cost)\n",
    "        \n",
    "        for key in error_keys:\n",
    "            prediction[key] = session.run(predict[key], \\\n",
    "                feed_dict= {x[key]: test_data.input[key]})\n",
    "        if (verbose): print(' session ends.')\n",
    "\n",
    "    if (verbose>1):\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    return num_logical_fault(prediction, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session begins  . . . . . . . . . .  session ends.\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "session begins  . . . . . . . . . .  session ends.\n"
     ]
    }
   ],
   "source": [
    "### Run an entire benchmark\n",
    "\n",
    "param= {}\n",
    "param['nn']= {}\n",
    "param['opt']= {}\n",
    "param['data']= {}\n",
    "param['usr']= {}\n",
    "param['nn']['num hidden']= 100\n",
    "param['nn']['W std']= 10.0**(-1.02861985)\n",
    "param['nn']['b std']= 0.0\n",
    "param['opt']['batch size']= 1000\n",
    "param['opt']['learning rate']= 10.0**(-3.84178815)\n",
    "param['opt']['iterations']= 10\n",
    "param['opt']['momentum']= 0.99\n",
    "param['opt']['decay']= 0.98\n",
    "param['data']['test fraction']= 0.1\n",
    "param['usr']['verbose']= True\n",
    " \n",
    "verbose= param['usr']['verbose']\n",
    "test_fraction= param['data']['test fraction']\n",
    "output= []\n",
    "num_classes= 2\n",
    "num_inputs= 2\n",
    "input_size= 6\n",
    "\n",
    "datafolder= '../Data/CNOTLabPickle/e-04/'\n",
    "file_list= os.listdir(datafolder)\n",
    "\n",
    "for filename in file_list:\n",
    "\n",
    "#     with open(datafolder+ filename + '.dat', \"wb\") as output_file:\n",
    "#         print(\"Reading data from \" + filename)\n",
    "#         model= Model(datafolder+ filename + '.dat')\n",
    "#         pickle.dump(model, output_file)\n",
    "#         continue\n",
    "\n",
    "    with open(datafolder + filename, 'rb') as input_file:\n",
    "        m = pickle.load(input_file)\n",
    "    \n",
    "    batch_size= param['opt']['batch size']\n",
    "    n_batches = m.train_size // batch_size\n",
    "\n",
    "    avg= train(param, m.train_data, m.test_data, \\\n",
    "        num_classes, num_inputs, input_size, n_batches)\n",
    "\n",
    "    run_log= {}\n",
    "    run_log['data']= {}\n",
    "    run_log['opt']= {}\n",
    "    run_log['res']= {}\n",
    "    run_log['data']['path']= filename\n",
    "    run_log['data']['fault scale']= m.error_scale\n",
    "    run_log['data']['total data size']= m.total_size\n",
    "    run_log['data']['test set size']= m.test_size\n",
    "    run_log['opt']['batch size']= batch_size\n",
    "    run_log['opt']['number of batches']= n_batches\n",
    "    run_log['res']['p']= m.p\n",
    "    run_log['res']['lu avg']= m.lu_avg\n",
    "    run_log['res']['lu std']= m.lu_std\n",
    "    run_log['res']['nn avg'] = m.error_scale * avg\n",
    "    run_log['res']['nn std'] = 0\n",
    "    output.append(run_log)\n",
    "\n",
    "outfilename = strftime(\"%Y-%m-%d-%H-%M-%S\", localtime())\n",
    "f = open('../Reports/CNOTLab/' + outfilename + '.json', 'w')\n",
    "f.write(json.dumps(output, indent=2))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " new sample=[ -3.33948385  19.92985473  -0.79408851]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00100100147651\n",
      "new sample=[ -4.23937629  41.14349962  -1.64948209]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.000983001449959\n",
      "new sample=[ -6.  10.  -4.]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00167200246626\n",
      "new sample=[ -6.  10.   2.]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00147500217568\n",
      "new sample=[ -2.  50.   2.]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.0077950114979\n",
      "new sample=[ -2.  50.  -4.]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00190900281584\n",
      "new sample=[ -6.  50.   2.]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00148900219633\n",
      "new sample=[ -6.  50.  -4.]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00167200246626\n",
      "new sample=[ -2.  10.  -4.]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.0014260021034\n",
      "new sample=[ -2.  10.   2.]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00147100216978\n",
      "new sample=[ -5.99965418  29.28591448  -0.88214752]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00166600245741\n",
      "new sample=[ -4.05108679  28.63601393  -4.        ]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.000981001447009\n",
      "new sample=[ -4.23864391  29.75962576   1.99992249]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00100800148684\n",
      "new sample=[ -2.00007984  31.67475624  -1.56075187]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00307900454163\n",
      "new sample=[ -4.04565679  10.00326669   1.99987169]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00100200147799\n",
      "new sample=[ -4.03378295  10.          -3.99938572]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.000984001451434\n",
      "new sample=[ -5.99958731  10.00364225  -1.0160734 ]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00168500248543\n",
      "new sample=[ -3.94859748  49.99951848  -3.99947943]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.000982001448484\n",
      "new sample=[ -4.25655523  49.99969038   1.99986281]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00103400152519\n",
      "new sample=[ -5.99962829  49.99953687  -0.80968704]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00166100245003\n",
      "new sample=[ -2.          10.00315791  -1.0257732 ]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00142500210192\n",
      "new sample=[ -2.          30.03880106  -3.99855365]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00485400715982\n",
      "new sample=[ -5.99958633  29.95328363  -3.99937711]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00166700245888\n",
      "new sample=[ -2.82623912  49.99941189  -1.09221226]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00113100166826\n",
      "new sample=[ -5.99966024  30.42732415   1.99991255]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00149500220518\n",
      "new sample=[ -4.94541605  18.76740507  -2.53471964]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00128500189542\n",
      "new sample=[ -4.9341622   42.59739497   0.74581193]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00129900191607\n",
      "new sample=[ -2.39381574  24.73218869   1.99987891]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00148800219485\n",
      "new sample=[ -5.09519117  17.53767024   0.78842787]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00139700206062\n",
      "new sample=[ -3.01680972  10.00361691  -2.61486594]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00105500155616\n",
      "new sample=[ -3.21032566  37.91599911   0.56897202]\n",
      "session begins  . . . . . . . . . .  session ends.\n",
      "result= 0.00100700148536\n",
      "new sample=[ -2.97279021  10.00366449   0.711255  ]\n",
      "session begins  . . . . ."
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5f75f6247c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper_bound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mmvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Result\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"at\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running time:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/dist-packages/bayesoptmodule.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         min_val, x_out, error = bo.optimize(self.evaluateSample, self.n_dim,\n\u001b[1;32m     87\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                                             self.params)\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mbayesopt.pyx\u001b[0m in \u001b[0;36mbayesopt.optimize (/home/iqclabk/Pooya/bayesopt/python/bayesopt.cpp:3446)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mbayesopt.pyx\u001b[0m in \u001b[0;36mbayesopt.raise_problem (/home/iqclabk/Pooya/bayesopt/python/bayesopt.cpp:2953)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import bayesopt\n",
    "from bayesoptmodule import BayesOptContinuous, BayesOptDiscrete\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class BayesOptTest(BayesOptContinuous):\n",
    "\n",
    "\n",
    "    def __init__(self, N, m, param):\n",
    "        super(BayesOptTest, self).__init__(N)\n",
    "        self.model = m\n",
    "        self.param = param\n",
    "        self.data_size= m.data_size\n",
    "\n",
    "        batch_size= self.param['opt']['batch size']\n",
    "        self.n_batches = self.model.train_size // batch_size\n",
    "        \n",
    "        self.num_classes= 2\n",
    "        self.num_inputs= 2\n",
    "        self.input_size= 6\n",
    "\n",
    "    def evaluateSample(self, x):\n",
    "        \n",
    "        print('new sample='+ str(x))\n",
    "\n",
    "        # self.param['opt']['batch size']= int(x[0] * 100) \n",
    "        self.param['opt']['learning rate']= 10**x[0] \n",
    "#         self.param['opt']['iterations']= int(x[1] * 10)\n",
    "        # self.param['opt']['momentum']= x[1] \n",
    "        # self.param['opt']['decay']= x[1]\n",
    "        self.param['nn']['num hidden']= int(x[1] * 10)\n",
    "        self.param['nn']['W std']= 10**x[2]\n",
    "        # self.param['nn']['b std']= 10**x[3]\n",
    "\n",
    "        avg= train(self.param, self.model.train_data, self.model.test_data, \\\n",
    "            self.num_classes, self.num_inputs, self.input_size, self.n_batches)\n",
    "        \n",
    "        print('result= '+ str(self.model.error_scale * avg))\n",
    "        return self.model.error_scale * avg\n",
    "    \n",
    "    ### Run hypertuning on the single file.\n",
    "\n",
    "\n",
    "lb = np.array([-6, 10, -4.])\n",
    "ub = np.array([-2, 50, 2.])\n",
    "\n",
    "param= {}\n",
    "param['nn']= {}\n",
    "param['opt']= {}\n",
    "param['data']= {}\n",
    "param['usr']= {}\n",
    "param['nn']['num hidden']= 500\n",
    "param['nn']['W std']= 0.1\n",
    "param['nn']['b std']= 0.0\n",
    "param['opt']['batch size']= 1000\n",
    "param['opt']['learning rate']= 0.00001\n",
    "param['opt']['iterations']= 10\n",
    "param['opt']['momentum']= 0.99\n",
    "param['opt']['decay']= 0.98\n",
    "param['data']['test fraction']= 0.1\n",
    "param['usr']['verbose']= True\n",
    "\n",
    "\n",
    "datafolder= '../Data/CNOTLabPickle/e-04/'\n",
    "filename= 'SyndromeAndError4.000e-04.txt.dat'\n",
    "\n",
    "with open(datafolder + filename, 'rb') as input_file:\n",
    "    model = pickle.load(input_file)\n",
    "\n",
    "hyperparam = {}\n",
    "hyperparam['n_iterations'] = 50\n",
    "hyperparam['n_iter_relearn'] = 5\n",
    "hyperparam['n_init_samples'] = 2\n",
    "hyperparam['noise']= 1e-10\n",
    "# hyperparam['l_type']= 'L_MCMC'\n",
    "hyperparam['kernel_name'] = \"kMaternARD5\"\n",
    "hyperparam['kernel_hp_mean'] = [1]\n",
    "hyperparam['kernel_hp_std'] = [5]\n",
    "hyperparam['surr_name'] = \"sStudentTProcessNIG\"\n",
    "#hyperparam['crit_name'] = \"cMI\"\n",
    "\n",
    "engine = BayesOptTest(len(lb), model, param)\n",
    "engine.parameters = hyperparam\n",
    "engine.lower_bound = lb\n",
    "engine.upper_bound = ub\n",
    "start = clock()\n",
    "mvalue, x_out, error = engine.optimize()\n",
    "print(\"Result\", mvalue, \"at\", x_out)\n",
    "print(\"Running time:\", clock() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
